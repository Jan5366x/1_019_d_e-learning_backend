stages:
  - test
  - build
  - build_container
  - deploy
  - cit

# skeleton to build container
.build_container_skeleton:
  image:
    name: gcr.io/kaniko-project/executor:debug
    entrypoint: [""]
  script:
    - |
        echo "{
            \"auths\": {
                \"$CI_REGISTRY\": {
                    \"username\": \"$CI_REGISTRY_USER\",
                    \"password\": \"$CI_REGISTRY_PASSWORD\"
                }
            }
        }" > /kaniko/.docker/config.json &&
        /kaniko/executor\
            --context $CI_PROJECT_DIR \
            --dockerfile $CI_PROJECT_DIR/dockerfile \
            --destination $CI_REGISTRY_IMAGE:$REGISTRY_IMAGE_TAG

# prepare deploy image
.deployer: &deployer
  image: debian:buster-slim
  stage: deploy
  before_script:
    - export DEBIAN_FRONTEND="noninteractive"
    - apt-get update -qq
    - apt-get install -y openssh-client curl jq
    - eval $(ssh-agent -s)
    - echo "$SSH_PRIVATE_KEY" | tr -d '\r' | ssh-add -
    - mkdir -p ~/.ssh
    - chmod 700 ~/.ssh

test:
  image: node:latest
  stage: test
  before_script:
  script:
    - echo "implement tests..."

# Heavy build environment ...
build:
  image: debian:buster-slim
  stage: build
  before_script:
    - export DEBIAN_FRONTEND="noninteractive"
    - apt-get update -qq
    - apt-get install -y nodejs npm
    # used to update some varaibles during the build
    #- sed -i "s#\$SECRET_ACCESS_KEY#${SECRET_ACCESS_KEY}#" app/documentFile/controllers.ts
    #- sed -i "s#\$ACCESS_KEY#${ACCESS_KEY}#" app/documentFile/controllers.ts
  script:
    - npm i
    # workaround to get it running ...
    - npm install bcrypt
    - npm run tsc
  artifacts:
    paths:
      - build/
      - node_modules

# Not working, due to some issues with bcrypt      
#build:
#  image: node:latest
#  stage: build
#  script:
#    - npm i
#    - npm run tsc
#  artifacts:
#    paths:
#      - build/
#      - node_modules/

# Taged with :live
build_container:
  extends: .build_container_skeleton
  stage: build_container
  dependencies:
    - build
  variables:
    REGISTRY_IMAGE_TAG: "latest" # should be changed to live for production
  only:
    - master
    - live
    - ci

# @TODO: Should be changed to live when development is finished.
# deploy image to the EC2
deploy:
  <<: *deployer
  stage: deploy
  variables:
    HOST: "ec2-18-184-245-52.eu-central-1.compute.amazonaws.com"
    TAG: "latest" # should be changed to live for production
  only:
    - master
    - ci
  script:
    - ssh ubuntu@${HOST} -o StrictHostKeyChecking=no "docker --version"
    - ssh ubuntu@${HOST} -o StrictHostKeyChecking=no "docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY"
    - ssh ubuntu@${HOST} -o StrictHostKeyChecking=no "docker stop $CI_PROJECT_NAME || true"
    - ssh ubuntu@${HOST} -o StrictHostKeyChecking=no "docker rm $CI_PROJECT_NAME || true"
    - ssh ubuntu@${HOST} -o StrictHostKeyChecking=no "docker pull $CI_REGISTRY_IMAGE:$TAG"
    - ssh ubuntu@${HOST} -o StrictHostKeyChecking=no "docker run -p 3000:3000 -d --name $CI_PROJECT_NAME $CI_REGISTRY_IMAGE:$TAG"
    - ssh ubuntu@${HOST} -o StrictHostKeyChecking=no "docker inspect -f '{{.State.Running}}' $CI_PROJECT_NAME"
    # Check if service is up
    - curl -m 1 --request GET ${HOST}:3000/version | jq
  when: manual
